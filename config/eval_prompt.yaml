kt_criteria_prompt: |
  role: 당신은 대규모 언어 모델(LLM)이 생성한 답변의 품질을 평가하는 전문가입니다. 제공된 input data의 passage를 바탕으로 LLM의 답변 품질을 정확하게 평가해 주세요.

  instruction:
  - LLMResult는 주어진 passage를 분석하여 질문에 대한 답변을 제시한 것입니다. 이때, passage는 question에 대응하는 답변을 도출하기 위한 핵심 정보를 담고 있습니다.
  - 일부의 passage는 question과 직접적인 관련이 없는 내용도 포함할 수 있음에 유의하세요.
  - ExpectedResult는 전문가들이 작성한 예상 정답지입니다. 이 내용을 참고하세요.
  - question에 대한 LLMResult를 면밀히 검토해, 아래 5개의 항복별로 각각 해당하는 점수를 정수로 부여해 주세요. 
  5개의 평가항목에 대해 Accuracy, Consistency, Understanding, Completeness, Requirements 평가하고, 이들의 곱해서 OverallScore(0점-10점)를 계산해주세요. OverallScore = Accuracy * Consistency * Understanding * Completeness * Requirements
  또한, 개별 항목별로 점수를 부여한 이유에 대해 각각 AccuracyReason, ConsistencyReason, UnderstandingReason, CompletenessReason, RequirementsReason에 설명해 주세요. OverallScore를 부여한 이유에 대해 OverallReason에 설명해 주세요.

      1. Accuracy(0-10점): LLM 답변에서 제공하는 정보가 정확한지를 확인합니다. 틀린 정보를 제공하거나 모른다는 답변은 피해야 합니다. 
          - ExpectedResult에서 제공하는 정보와 비교해, 틀린 정보를 포함하지는 않는가?
          - 오직 제공된 passage의 정보를 바탕으로 답변이 생성되었는가?
          - 질문과 관련된 passage(s)을 통해 정보를 잘 추출하였으며, 관련 없는 passage의 정보는 활용되지 않았는가?
          - 잘못된 정보 없이 passage와 일치 하는가?
          - 수치, 지역명, 상품명, 공고명 등 구체적 정보가 정확한가?
          
      2. Consistency(0점, 1점): LLM 답변이 일관성 있는 정보를 제공하는지 확인해야 합니다. 정보가 상호모순되거나 약간이라도 잘못된 정보를 포함하면 안됩니다.
          - 내부 문장 간 논리적 모순이나 상충 내용 없는가?
          - 문맥에 어긋나는 문장이나 문단은 없는가?

      3. Understanding(0점, 1점): 질문의 문맥을 이해하고 그에 맞는 LLM 답변을 제공하는 능력이 중요합니다. 문맥을 고려하지 않고 일반적인 답변만을 제공하면 안됩니다.
          - 질문의 의도를 정확하게 파악했는가?
          - 질문과 관련된 대답을 하고 있는가?
          - 질문에 대한 정보를 모두 포함하고 있고, 틀리지 않았는가?
      
      4. Completeness(0점, 1점): LLM 답변이 주어진 질문에 충분한 정보를 포함하는지 여부가 중요합니다. 질문이 여러개 이면, 여러 질문에 대해 모두 답변해야 합니다.
          - ExpectedResult의 내용을 모두 포함하고 있는가?
          - 불필요한 부연 설명 없이 직접적인 답변 제공하는가?    

      5. Requirements(0점, 1점): 다음의 요구사항은 반드시 만족시켜야 하는 조건입니다. 아래 조건을 만족하지 못하면, 개별 점수와 상관없이 OverallScore를 0점을 부여합니다.
          - 질문에 대한 답변이 추상적이지 않고, 명확하고 구체적인가?
          - 주어진 정보로 알 수 없다고 대답하거나, 답변을 회피하지 않는가?

  결과는 한국어로 생성해주고, 다음 template에 맞게 jsonl 형식으로 결과를 생성해주세요.
          '''  
                  
          template = '{"OverallScore" : "", "OverallReason" : "", "Score" : [{"AccuracyScore":"", "AccuracyReason":""},{"ConsistencyScore":"", "ConsistencyReason":""},{"UnderstandingScore":"", "UnderstandingReason":""},{"CompletenessScore":"", "CompletenessReason":""},{"RequirementsScore":"", "RequirementsReason":""}]}'
          # template = '''{"LLMResult":[{"TotalScore":""},{"Accuracy":""},{"Consistency":""},{"Understanding":""},{"Completeness":""},{"Reason":""}]}'''
          # template = '''{"LLMResult":[{"TotalScore":""},{"Reason":""}]}'''
          

multi_criteria_prompt: |
  다음 응답을 아래 평가 기준별로 모두 평가하고 가중치를 적용한 총점을 0-1 사이에 소수점 3자리까지 점수를 넣어주세요.
  
  [원래 프롬프트]
  {{prompt}}
  
  [모델 응답]
  {{response}}
  {% if reference_answer %}
  [참조 답변]
  {{reference_answer}}
  {% endif %}
  [평가 기준]
  {% for c in criteria_list %}
  {{loop.index}}. {{c.name}}: {{c.description}} (최대 점수: {{c.max_score}}) (가중치: {{c.weight}}) 
  {% endfor %}
  
  각 기준별로 아래의 JSON형식으로 제공해 주세요.
  {
      "기준": 평가 기준,
      "점수": 점수,
      "피드백": 피드백,   
      "총점": 총점 
  }

single_criteria_prompt: |
  다음 응답을 평가해주세요:
  
  원래 프롬프트:
  {{prompt}}
  
  모델 응답:
  {{response}}
  
  평가 기준: {{criteria.name}}
  설명: {{criteria.description}}
  최대 점수: {{criteria.max_score}}
  
  다음 형식으로 평가 결과를 제공해주세요:
  점수: [0-{{criteria.max_score}} 사이의 숫자]
  피드백: [상세한 평가 설명]
  {% if reference_answer %}
  참조 답변:
  {{reference_answer}}
  {% endif %} 

