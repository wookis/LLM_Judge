kt_criteria_prompt: |
  role: 당신은 대규모 언어 모델(LLM)이 생성한 답변의 품질을 평가하는 전문가입니다. 제공된 input data의 passage를 바탕으로 LLM의 답변 품질을 정확하게 평가해 주세요.

  instruction:
  - LLMResult는 주어진 passage를 분석하여 질문에 대한 답변을 제시한 것입니다. 이때, passage는 question에 대응하는 답변을 도출하기 위한 핵심 정보를 담고 있습니다.
  - 일부의 passage는 question과 직접적인 관련이 없는 내용도 포함할 수 있음에 유의하세요.
  - ExpectedResult는 전문가들이 작성한 예상 정답지입니다. 이 내용을 참고하세요.
  - question에 대한 LLMResult를 면밀히 검토해, 아래 5개의 항복별로 각각 해당하는 점수를 정수로 부여해 주세요. 
  5개의 평가항목에 대해 Accuracy, Consistency, Understanding, Completeness, Requirements 평가하고, 이들의 곱해서 OverallScore(0점-10점)를 계산해주세요. OverallScore = Accuracy * Consistency * Understanding * Completeness * Requirements
  또한, 개별 항목별로 점수를 부여한 이유에 대해 각각 AccuracyReason, ConsistencyReason, UnderstandingReason, CompletenessReason, RequirementsReason에 설명해 주세요. OverallScore를 부여한 이유에 대해 OverallReason에 설명해 주세요.

      1. Accuracy(0-10점): LLM 답변에서 제공하는 정보가 정확한지를 확인합니다. 틀린 정보를 제공하거나 모른다는 답변은 피해야 합니다. 
          - 틀린 정보를 포함하지는 않는가?
          - 오직 제공된 passage의 정보를 바탕으로 답변이 생성되었는가?
          - 질문과 관련된 passage(s)을 통해 정보를 잘 추출하였으며, 관련 없는 passage의 정보는 활용되지 않았는가?
          - 잘못된 정보 없이 passage와 일치 하는가?
          - 수치, 지역명, 상품명, 공고명 등 구체적 정보가 정확한가?
          
      2. Consistency(0점, 1점): LLM 답변이 일관성 있는 정보를 제공하는지 확인해야 합니다. 정보가 상호모순되거나 약간이라도 잘못된 정보를 포함하면 안됩니다.
          - 내부 문장 간 논리적 모순이나 상충 내용 없는가?
          - 문맥에 어긋나는 문장이나 문단은 없는가?

      3. Understanding(0점, 1점): 질문의 문맥을 이해하고 그에 맞는 LLM 답변을 제공하는 능력이 중요합니다. 문맥을 고려하지 않고 일반적인 답변만을 제공하면 안됩니다.
          - 질문의 의도를 정확하게 파악했는가?
          - 질문과 관련된 대답을 하고 있는가?
          - 질문에 대한 정보를 모두 포함하고 있고, 틀리지 않았는가?
      
      4. Completeness(0점, 1점): LLM 답변이 주어진 질문에 충분한 정보를 포함하는지 여부가 중요합니다. 질문이 여러개 이면, 여러 질문에 대해 모두 답변해야 합니다.
          - ExpectedResult의 내용을 모두 포함하고 있는가?
          - 불필요한 부연 설명 없이 직접적인 답변 제공하는가?    

      5. Requirements(0점, 1점): 다음의 요구사항은 반드시 만족시켜야 하는 조건입니다. 아래 조건을 만족하지 못하면, 개별 점수와 상관없이 OverallScore를 0점을 부여합니다.
          - 질문에 대한 답변이 추상적이지 않고, 명확하고 구체적인가?
          - 주어진 정보로 알 수 없다고 대답하거나, 답변을 회피하지 않는가?

  결과는 한국어로 생성해주고, 다음 template에 맞게 jsonl 형식으로 결과를 생성해주세요.
          '''  
                  
          template = '{"OverallScore" : "", "OverallReason" : "", "Score" : [{"AccuracyScore":"", "AccuracyReason":""},{"ConsistencyScore":"", "ConsistencyReason":""},{"UnderstandingScore":"", "UnderstandingReason":""},{"CompletenessScore":"", "CompletenessReason":""},{"RequirementsScore":"", "RequirementsReason":""}]}'
          # template = '''{"LLMResult":[{"TotalScore":""},{"Accuracy":""},{"Consistency":""},{"Understanding":""},{"Completeness":""},{"Reason":""}]}'''
          # template = '''{"LLMResult":[{"TotalScore":""},{"Reason":""}]}'''
          

multi_criteria_prompt: |
  다음 응답을 아래 평가 기준별로 모두 평가하고 가중치를 적용한 총점을 0-1 사이에 소수점 3자리까지 점수를 넣어주세요.
  
  [원래 프롬프트]
  {{prompt}}
  
  [모델 응답]
  {{response}}
  {% if reference_answer %}
  [참조 답변]
  {{reference_answer}}
  {% endif %}
  [평가 기준]
  {% for c in criteria_list %}
  {{loop.index}}. {{c.name}}: {{c.description}} (최대 점수: {{c.max_score}}) (가중치: {{c.weight}}) 
  {% endfor %}

  각 기준별로 아래의 JSON형식으로 제공해 주세요.
  {
      "기준": 평가 기준,
      "점수": 점수,
      "피드백": 피드백,   
      "총점": 총점 
  }


single_criteria_prompt: |
  [role]
  당신은 대규모 언어 모델(LLM)이 생성한 답변의 품질을 평가하는 전문가입니다. 
  제공된 원래 프롬프트에 대한 모델응답을 바탕으로 LLM의 답변 품질을 정확하게 평가해 주세요.
 
  [원래 프롬프트]
  {{prompt}}
  
  [모델응답]
  {{response}}
  
  [instruction]
   - 프롬프트와 모델응답을 면밀히 검토해, 아래 5개의 항복별로 각각 해당하는 점수를 소수점 3자리(ex. 0.832) 수치로 부여해 주세요. 
   - 5개의 평가항목에 대해 Accuracy, Consistency, Understanding, Completeness, Requirements 평가
   - OverallScore = Accuracy + Consistency + Understanding + Completeness + Requirements
   - OverallScore를 부여한 이유에 대해 OverallReason에 설명해 주세요. 
   - OverallScore 계산식은 변경하지 말고 정확히 해주세요.


      1. Accuracy: LLM 답변에서 제공하는 정보가 정확한지를 확인합니다. 틀린 정보를 제공하거나 모른다는 답변은 피해야 합니다.(가중치:1.5)
          - 틀린 정보를 포함하지는 않는가?
          - 정확한 정보를 바탕으로 답변이 생성되었는가?
          - 관련 없는 정보는 활용되지 않았는가?
          - 수치, 지역명, 상품명, 공고명 등 구체적 정보가 정확한가?
          
      2. Consistency: LLM 답변이 일관성 있는 정보를 제공하는지 확인해야 합니다. 정보가 상호모순되거나 약간이라도 잘못된 정보를 포함하면 안됩니다.(가중치:1.0)
          - 내부 문장 간 논리적 모순이나 상충 내용 없는가?
          - 문맥에 어긋나는 문장이나 문단은 없는가?

      3. Understanding: 질문의 문맥을 이해하고 그에 맞는 LLM 답변을 제공하는 능력이 중요합니다. 문맥을 고려하지 않고 일반적인 답변만을 제공하면 안됩니다.(가중치:1.3)
          - 질문의 의도를 정확하게 파악했는가?
          - 질문과 관련된 대답을 하고 있는가?
          - 질문에 대한 정보를 모두 포함하고 있고, 틀리지 않았는가?
      
      4. Completeness: LLM 답변이 주어진 질문에 충분한 정보를 포함하는지 여부가 중요합니다. 질문이 여러개 이면, 여러 질문에 대해 모두 답변해야 합니다.(가중치:1.0)
          - 질문의 내용을 모두 포함하고 있는가?
          - 불필요한 부연 설명 없이 직접적인 답변 제공하는가?    

      5. Requirements: 다음의 요구사항은 반드시 만족시켜야 하는 조건입니다. (가중치:0.5)
          - 질문에 대한 답변이 추상적이지 않고, 명확하고 구체적인가?
          - 주어진 정보로 알 수 없다고 대답하거나, 답변을 회피하지 않는가?

    [제약 조건] 
    피드백 코멘트는 사용자 질문과 동일한 언어로 작성해야 합니다.
    설명은 간결하고 객관적이어야 합니다.
    JSON 출력 외에 설명이나 추론을 포함하지 마세요.

    [출력 형식]
    오직 JSON 객체만 반환합니다(추가 키나 설명을 추가하지 마세요).

    ```json
    {
    Accuracy: score,
    Consistency: score,
    Understanding: score,
    Completeness: score,
    Requirements: score,
    OverallScore : score,
    OverallScoreReason : <brief reason summary>
    }




other_criteria_prompt: |
    # Role
    You are an impartial expert AI judge for evaluating assistant chat completions.

    # Task
    Evaluate the quality of the assistant's last response based on the provided conversation history.

    # Inputs
    - **messages**: The full conversation history (list of role/content pairs).
    - **assistant_response**: The model's answer to the last user message.

    # Evaluation Criteria
    Evaluate according to the following four criteria:

    ## 1. Accuracy (0‑3)
    - 0: completely incorrect
    - 1: partially correct but with errors
    - 2: mostly correct
    - 3: entirely correct

    ## 2. Relevance (0‑2)
    - 0: irrelevant
    - 1: partially relevant
    - 2: fully relevant

    ## 3. Clarity (0‑2)
    - 0: unintelligible
    - 1: understandable but awkward
    - 2: clear and natural

    ## 4. Completeness (0‑3)
    - 0: no content addressing request
    - 1: addresses only part
    - 2: mostly covers
    - 3: fully satisfies all requirements

    # Constraints
    The comments field must be written in the same language as the user's question.
    Be concise and objective in your explanation.
    Do not include explanations or reasoning outside of the JSON output.

    # Output Format
    Return **only** the following JSON object (do not add any extra keys or explanations):

    ```json
    {
    "accuracy": <0‑3>,
    "relevance": <0‑2>,
    "clarity": <0‑2>,
    "completeness": <0‑3>,
    "comments": "<brief reason summary>"
    }


otherh_criteria_prompt: |
    # 역할
    당신은 어시스턴트 채팅 완료를 평가하는 공정한 전문가 AI 심사위원입니다.

    # 작업
    제공된 대화 기록을 기반으로 어시스턴트의 마지막 응답 품질을 평가합니다.

    # 입력
    - **messages**: 전체 대화 기록(역할/콘텐츠 쌍 목록).
    - **assistant_response**: 마지막 사용자 메시지에 대한 모델의 답변.

    # 평가 기준
    다음 네 가지 기준에 따라 평가하십시오.

    ## 1. 정확성 (0~3)
    - 0: 완전히 틀림
    - 1: 부분적으로 맞지만 오류가 있음
    - 2: 대부분 맞음
    - 3: 완전히 맞음

    ## 2. 관련성 (0~2)
    - 0: 무관
    - 1: 부분적으로 맞음
    - 2: 완전히 맞음

    ## 3. 명확성 (0~2)
    - 0: 이해할 수 없음
    - 1: 이해할 수 있지만 어색함
    - 2: 명확하고 자연스러움

    ## 4. 완전성 (0~3)
    - 0: 콘텐츠 주소 요청 없음
    - 1: 일부만 다룸
    - 2: 대부분 다룸
    - 3: 모든 요구 사항을 완전히 충족

    # 제약 조건
    댓글 필드는 사용자 질문과 동일한 언어로 작성해야 합니다.
    설명은 간결하고 객관적이어야 합니다.
    JSON 출력 외에 설명이나 추론을 포함하지 마세요.

    # 출력 형식
    다음 JSON 객체 **만** 반환합니다(추가 키나 설명을 추가하지 마세요).

    ```json
    {
    "accuracy": <0‑3>,
    "relevance": <0‑2>,
    "clarity": <0‑2>,
    "completeness": <0‑3>,
    "comments": "<간략한 이유 요약>"
    }
